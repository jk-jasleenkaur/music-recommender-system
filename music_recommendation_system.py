# -*- coding: utf-8 -*-
"""Copy of MUSIC RECOMMENDATION SYSTEM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDDo2jPPFLs6rQMAtZ6nyhFK16-VZDaw
"""

import pandas as pd

import csv

input_file = "spotify_millsongdata.csv"
output_file = "cleaned_spotify_millsongdata.csv"

# Read the file line by line and clean it
with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:
    reader = csv.reader(infile)
    writer = csv.writer(outfile)

    for row in reader:
        # Check for unclosed quotes and fix them
        if row.count('"') % 2 != 0:
            # Fix the row (e.g., by closing the quotes or concatenating with the next line)
            # This is a basic example; more sophisticated handling may be needed
            row = [cell.replace('"', '') for cell in row]

        writer.writerow(row)

# Load the cleaned CSV file into a DataFrame
df = pd.read_csv(output_file)

df.head(5)

df.tail(5)

df.shape

df.isnull().sum()

df =df.sample(5000).drop('link', axis=1).reset_index(drop=True)

df.head(10)

df['text'][0]

df.shape

df['text'] = df['text'].str.lower().replace(r'^\w\s', ' ').replace(r'\n', ' ', regex = True)

import nltk
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

def tokenization(txt):
    tokens = nltk.word_tokenize(txt)
    stemming = [stemmer.stem(w) for w in tokens]
    return " ".join(stemming)

nltk.download('punkt')

# Assuming you have already defined the tokenization function
def tokenization(text):
    from nltk.tokenize import word_tokenize
    return word_tokenize(text)

# Download the 'punkt' resource
nltk.download('punkt')

# Assuming df is your DataFrame and 'text' is the column you want to tokenize
df['text'] = df['text'].apply(lambda x: tokenization(x))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

if 'text' not in df.columns:
    raise KeyError("'text' column not found in the DataFrame")

# Check for NaN values and handle them (e.g., by filling with an empty string)
df['text'] = df['text'].fillna('')

# Ensure all entries in 'text' are strings
df['text'] = df['text'].astype(str)

# Create the TfidfVectorizer
tfidvector = TfidfVectorizer(analyzer='word', stop_words='english')

# Fit and transform the text data
matrix = tfidvector.fit_transform(df['text'])

# Compute cosine similarity
similarity = cosine_similarity(matrix)

print(similarity)

similarity[0]

df[df['song'] == 'Amazing']

def recommendation(song_df):
    idx = df[df['song'] == song_df].index[0]
    distances = sorted(list(enumerate(similarity[idx])),reverse=True,key=lambda x:x[1])

    songs = []
    for m_id in distances[1:21]:
        songs.append(df.iloc[m_id[0]].song)

    return songs

recommendation('Amazing')

import pickle
pickle.dump(similarity,open('similarity.pkl','wb'))
pickle.dump(df,open('df.pkl','wb'))

